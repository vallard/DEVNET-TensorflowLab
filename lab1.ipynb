{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEVNET Tensorflow GPU Matrix Lab\n",
    "\n",
    "Welcome to the DEVNET Tensorflow lab!  In this lab our goal is to use Tensorflow to show you how GPUs help improve performance of data science applications. There are several introductions to Tensorflow included in this Jupyter Notebook. In this walkthrough we will explain some of the benefits of GPUs and show how Tensorflow works on them!  Let's start by loading some of the libraries we will be using for this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the Tensorflow library.  Tensorflow is built to numerical computation using data graphs.  \n",
    "import tensorflow as tf\n",
    "# Then we load NumPy.  NumPy is a library built for matrix functions on Python. \n",
    "import numpy as np\n",
    "# Finally, import the timer so we can time how long it takes to run operations. \n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Making Matrix Operations Faster\n",
    "\n",
    "GPUS were intended to render graphics on computers for games and CAD software.  To do this efficiently and fast the\n",
    "idea was to get a ton of processor cores and make them extremely good at doing matrix operations.  Things were humming along nicely until Machine Learning and Crypto currency started gaining steam.  It turns out that matrix operations\n",
    "are exactly what is needed to improve performance of machine learning as well as proof of work for crypto\n",
    "currencies.  \n",
    "In the code below we will launch tensorflow and then compare the time it takes to perform some matrix operations on the GPU and the CPU.  Don't get hung up on the syntax.  The point of this exercise is to show you the huge improvements in performance we can get by running Tensorflow on a GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Exercise:  Run the below tensorflow on GPUs and on a CPU and compare the difference\n",
    "\n",
    "In the code below we load Tensorflow and some other python libraries.  Normally when we start creating a Tensorflow graph we just use the default device.  However, we can tell what device we want the graph to run on by specifying it with the ```with tf.device()``` directive.  In the code below try two different kinds of devices: ```/gpu:0``` and ```/cpu:0```. Be sure to put them in quotes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "device() missing 1 required positional argument: 'device_name_or_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-43d6f5772fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The shape of our matrix is 15,000 x 15,000.  That is really big!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Try different devices: \"/cpu:0\" to \"/gpu:0\" and see the time difference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# create the computational graph.  This doesn't run it, it just gets it ready!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrandom_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdot_operation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: device() missing 1 required positional argument: 'device_name_or_function'"
     ]
    }
   ],
   "source": [
    "shape = (15000, 15000)  # The shape of our matrix is 15,000 x 15,000.  That is really big!\n",
    "with tf.device():   # Try different devices: \"/cpu:0\" to \"/gpu:0\" and see the time difference. \n",
    "    # create the computational graph.  This doesn't run it, it just gets it ready!\n",
    "    random_matrix = tf.random_uniform(shape=shape, minval=0, maxval=1)\n",
    "    dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "    sum_operation = tf.reduce_sum(dot_operation)\n",
    "    \n",
    "start = timer()\n",
    "# now start running the tensorflow computational graph to get the final answer. \n",
    "with tf.Session() as session:\n",
    "    result = session.run(sum_operation)\n",
    "duration = timer() - start\n",
    "print(\"Duration: {:0.2f} seconds\".format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you got TypeError: device() missing, be sure to fill in the value for ```tf.device()```.  \n",
    "\n",
    "The CPU time takes about 6.5 seconds.  The same operation on the GPU takes 1.7 seconds. While this example is pretty contrived, the take away is the same.  When we run operations on deep neural networks we are doing millions of these operations.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
